<div class="page-content">
  <div class="wrapper">
    <article class="post">
      <div class="post-content">
        <p class="center">
          <img style="width: 24rem; max-width:100%; border-radius: 50%;" alt="profile photo" src="/assets/photo.jpg">
        </p>
        <p>I am a research engineer at <a href="https://www.kakaoenterprise.com/">Kakao Enterprise</a>, where I work on generative models for sequence to sequence modeling. I am especially interested in designing efficient and expressive non-autoregressive generative models to attain significant inference time reduction for real-world scenarios.</p>
        <p>At <a href="https://www.kakaocorp.com/?lang=en">Kakao</a> and Kakao Enterprise I've worked on machine translation, image recognition and speech synthesis. I received my bachelor's degree in electrical and computer engineering at <a href="https://en.snu.ac.kr/index.html">Seoul National University</a>.</p>
        <p class="center">
          <a href="https://github.com/jaywalnut310">
              Github
          </a>
          &nbsp;/&nbsp;
          <a href="https://scholar.google.com/citations?user=-ZJaGikAAAAJ">
            Google Scholar
          </a>
          &nbsp;/&nbsp;
          <a href="/assets/cv.pdf">CV</a>
          &nbsp;/&nbsp;
          <a href="https://www.linkedin.com/in/jaywalnut310">
              LinkedIn
          </a>
          &nbsp;/&nbsp;
          <a href="mailto:jaywalnut310@gmail.com">
            Email
          </a>
        </p>
      </div>
    </article>
  </div>
  <div class="wrapper">
  <article class="post">
    <div class="post-content cv">
      <h2><strong>Research</strong></h2>
      <table>
        <tbody>
          <tr>
            <td class="col-first">
              <img src="assets/papers/vits.svg">
            </td>
            <td class="col-second">
              <a href="https://arxiv.org/abs/2106.06103">
                <span class="paper-title">Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</span>
              </a>
              <br>
              <strong>Jaehyeon Kim</strong>,
              Jungil Kong, 
              Juhee Son
              <br>
              <em>ICML</em>, 2021
              <br>
              <a href="https://github.com/jaywalnut310/vits">code</a> /
              <a href="https://jaywalnut310.github.io/vits-demo">demo</a>
              <p></p>
              We propose a method to train text-to-speech synthesis models in an end-to-end fashion. With the model, we can synthesize high-quality raw waveforms from text directly.
            </td>
          </tr>

          <tr>
            <td class="col-first">
              <img src="assets/papers/glowtts.svg">
            </td>
            <td class="col-second">
              <a href="https://arxiv.org/abs/2005.11129">
                <span class="paper-title">Glow-TTS: A Generative Flow for Text-to-Speech via Monotonic Alignment Search</span>
              </a>
              <br>
              <strong>Jaehyeon Kim</strong>,
              Sungwon Kim, 
              Jungil Kong, 
              <a href="http://data.snu.ac.kr/index.php/people/">Sungroh Yoon</a>
              <br>
              <em>NeurIPS</em>, 2020 &nbsp; <font color="#FF8080"><strong>(Oral presentation)</strong></font>
              <br>
              <a href="https://github.com/jaywalnut310/glow-tts">code</a> /
              <a href="https://jaywalnut310.github.io/glow-tts-demo">demo</a> /
              <a href="https://nips.cc/virtual/2020/public/poster_5c3b99e8f92532e5ad1556e53ceea00c.html">presentation</a>
              <p></p>
              We propose a method to search the most likely monotonic alignment between text and speech, and train a flow-based generative model upon it. With the model, we can synthesize high-quality mel-spectrogram frames in parallel.
            </td>
          </tr>
          <tr>
            <td class="col-first">
              <img src="assets/papers/hifigan.svg">
            </td>
            <td class="col-second">
              <a href="https://arxiv.org/abs/2010.05646">
                <span class="paper-title">HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis</span>
              </a>
              <br>
              Jungil Kong, 
              <strong>Jaehyeon Kim</strong>,
              Jaekyoung Bae
              <br>
              <em>NeurIPS</em>, 2020
              <br>
              <a href="https://github.com/jik876/hifi-gan">code</a> /
              <a href="https://jik876.github.io/hifi-gan-demo/">demo</a>
              <p></p>
              We propose modules for generators and discriminators in generative adversarial networks that caputre periodic patterns in raw audio. Our model can synthesize high-quality raw waveforms from mel-spectrograms in parallel. 
            </td>
          </tr>
          <tr>
            <td class="col-first">
              <img src="assets/papers/flowavenet.png">
            </td>
            <td class="col-second">
              <a href="https://arxiv.org/abs/1811.02155">
                <span class="paper-title">FloWaveNet : A Generative Flow for Raw Audio</span>
              </a>
              <br>
              Sungwon Kim, 
              Sang-gil Lee, 
              Jongyoon Song,
              <strong>Jaehyeon Kim</strong>,
              <a href="http://data.snu.ac.kr/index.php/people/">Sungroh Yoon</a>
              <br>
              <em>ICML</em>, 2019
              <br>
              <a href="https://github.com/ksw0306/FloWaveNet">code</a> /
              <a href="https://ksw0306.github.io/flowavenet-demo/">demo</a>
              <p></p>
              We propose a flow-based generative model for speech synthesis. Our model can transform random normal noise to raw waveforms given mel-spectrograms in parallel.
            </td>
          </tr>
        </tbody>
      </table>
    </div>
  </article>
  </div>
</div>
