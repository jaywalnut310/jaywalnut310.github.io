<div class="page-content">
  <div class="wrapper">
    <article class="post">
      <div class="post-content">
        <p class="center">
          <img
            style="width: 24rem; max-width: 100%; border-radius: 50%"
            alt="profile photo"
            src="/assets/photo.jpg"
          />
        </p>
        <p>
          I am a deep learning researcher at
          <a href="https://www.krafton.com/en/">KRAFTON</a>, where I work on
          generative models for Text-to-Speech. I am especially interested in
          designing efficient and expressive generative models to be applicable
          in real-world scenarios.
        </p>
        <p>
          I received my bachelor's degree in electrical and computer engineering
          at
          <a href="https://en.snu.ac.kr/index.html">Seoul National University</a
          >.
        </p>
        <p class="center">
          <a href="https://github.com/jaywalnut310"> Github </a>
          &nbsp;/&nbsp;
          <a href="https://scholar.google.com/citations?user=-ZJaGikAAAAJ">
            Google Scholar
          </a>
          &nbsp;/&nbsp;
          <a href="/assets/cv.pdf">CV</a>
          &nbsp;/&nbsp;
          <a href="https://www.linkedin.com/in/jaywalnut310"> LinkedIn </a>
          &nbsp;/&nbsp;
          <a href="mailto:jaywalnut310@gmail.com"> Email </a>
        </p>
      </div>
    </article>
  </div>
  <div class="wrapper">
    <article class="post">
      <div class="post-content cv">
        <h2><strong>Research</strong></h2>
        <table>
          <tbody>
            <tr>
              <td class="col-first">
                <img src="assets/papers/clam-tts.svg" />
              </td>
              <td class="col-second">
                <a href="https://openreview.net/forum?id=ofzeypWosV">
                  <span class="paper-title"
                    >CLaM-TTS: Improving Neural Codec Language Model for
                    Zero-Shot Text-to-Speech</span
                  >
                </a>
                <br />
                <strong>Jaehyeon Kim</strong>,
                <a href="https://sites.google.com/view/keonlee9420">Keon Lee</a
                >,
                <a href="https://sites.google.com/view/sjchung"
                  >Seungjun Chung</a
                >,
                <a href="https://sites.google.com/view/jaewoongcho"
                  >Jaewoong Cho</a
                >
                <br />
                <em>ICLR</em>, 2024
                <br />
                <a href="https://clam-tts.github.io/">demo</a>
                <p></p>
                Probabilistic residual vector quantization and multi-token
                generation method for efficient neural codec language modeling.
              </td>
            </tr>
            <tr>
              <td class="col-first">
                <img src="assets/papers/qasa.png" />
              </td>
              <td class="col-second">
                <a href="https://openreview.net/forum?id=5ud0h8OXwD">
                  <span class="paper-title"
                    >QASA: Advanced Question Answering on Scientific
                    Articles</span
                  >
                </a>
                <br />
                Yoonjoo Lee, Kyungjae Lee, Sunghyun Park, Dasol Hwang,
                <strong>Jaehyeon Kim</strong>, Hong-in Lee, Moontae Lee
                <br />
                <em>ICML</em>, 2023
                <br />
                <a href="https://github.com/lgresearch/QASA">data</a>
                <p></p>
                QASA benchmark and approach for full-stack reasoning on
                scientific articles in AI and ML fields, highlighting the
                importance of rationale-generation for advanced QA.
              </td>
            </tr>
            <tr>
              <td class="col-first">
                <img src="assets/papers/progressive_deblurring.png" />
              </td>
              <td class="col-second">
                <a href="https://arxiv.org/abs/2207.11192">
                  <span class="paper-title"
                    >Progressive Deblurring of Diffusion Models for
                    Coarse-to-Fine Image Synthesis</span
                  >
                </a>
                <br />
                Sangyun Lee, Hyungjin Chung, <strong>Jaehyeon Kim</strong>, Jong
                Chul Ye
                <br />
                <em>NeurIPS Workshop on Score-Based Methods</em>, 2022
                <br />
                <a href="https://github.com/sangyun884/blur-diffusion">code</a>
                <p></p>
                Diffusion models incorporating a coarse-to-fine approach and a
                blur diffusion technique that considers the varying importance
                of frequency components.
              </td>
            </tr>
            <tr>
              <td class="col-first">
                <img src="assets/papers/vits.svg" />
              </td>
              <td class="col-second">
                <a href="https://arxiv.org/abs/2106.06103">
                  <span class="paper-title"
                    >Conditional Variational Autoencoder with Adversarial
                    Learning for End-to-End Text-to-Speech</span
                  >
                </a>
                <br />
                <strong>Jaehyeon Kim</strong>, Jungil Kong, Juhee Son
                <br />
                <em>ICML</em>, 2021
                <br />
                <a href="https://github.com/jaywalnut310/vits">code</a> /
                <a href="https://jaywalnut310.github.io/vits-demo">demo</a>
                <p></p>
                We propose a method to train text-to-speech synthesis models in
                an end-to-end fashion. With the model, we can synthesize
                high-quality raw waveforms from text directly.
              </td>
            </tr>

            <tr>
              <td class="col-first">
                <img src="assets/papers/glowtts.svg" />
              </td>
              <td class="col-second">
                <a href="https://arxiv.org/abs/2005.11129">
                  <span class="paper-title"
                    >Glow-TTS: A Generative Flow for Text-to-Speech via
                    Monotonic Alignment Search</span
                  >
                </a>
                <br />
                <strong>Jaehyeon Kim</strong>, Sungwon Kim, Jungil Kong,
                <a href="http://data.snu.ac.kr/index.php/people/"
                  >Sungroh Yoon</a
                >
                <br />
                <em>NeurIPS</em>, 2020 &nbsp;
                <font color="#FF8080"
                  ><strong>(Oral presentation)</strong></font
                >
                <br />
                <a href="https://github.com/jaywalnut310/glow-tts">code</a> /
                <a href="https://jaywalnut310.github.io/glow-tts-demo">demo</a>
                /
                <a
                  href="https://nips.cc/virtual/2020/public/poster_5c3b99e8f92532e5ad1556e53ceea00c.html"
                  >presentation</a
                >
                <p></p>
                We propose a method to search the most likely monotonic
                alignment between text and speech, and train a flow-based
                generative model upon it. With the model, we can synthesize
                high-quality mel-spectrogram frames in parallel.
              </td>
            </tr>
            <tr>
              <td class="col-first">
                <img src="assets/papers/hifigan.svg" />
              </td>
              <td class="col-second">
                <a href="https://arxiv.org/abs/2010.05646">
                  <span class="paper-title"
                    >HiFi-GAN: Generative Adversarial Networks for Efficient and
                    High Fidelity Speech Synthesis</span
                  >
                </a>
                <br />
                Jungil Kong,
                <strong>Jaehyeon Kim</strong>, Jaekyoung Bae
                <br />
                <em>NeurIPS</em>, 2020
                <br />
                <a href="https://github.com/jik876/hifi-gan">code</a> /
                <a href="https://jik876.github.io/hifi-gan-demo/">demo</a>
                <p></p>
                We propose modules for generators and discriminators in
                generative adversarial networks that caputre periodic patterns
                in raw audio. Our model can synthesize high-quality raw
                waveforms from mel-spectrograms in parallel.
              </td>
            </tr>
            <tr>
              <td class="col-first">
                <img src="assets/papers/flowavenet.png" />
              </td>
              <td class="col-second">
                <a href="https://arxiv.org/abs/1811.02155">
                  <span class="paper-title"
                    >FloWaveNet : A Generative Flow for Raw Audio</span
                  >
                </a>
                <br />
                Sungwon Kim, Sang-gil Lee, Jongyoon Song,
                <strong>Jaehyeon Kim</strong>,
                <a href="http://data.snu.ac.kr/index.php/people/"
                  >Sungroh Yoon</a
                >
                <br />
                <em>ICML</em>, 2019
                <br />
                <a href="https://github.com/ksw0306/FloWaveNet">code</a> /
                <a href="https://ksw0306.github.io/flowavenet-demo/">demo</a>
                <p></p>
                We propose a flow-based generative model for speech synthesis.
                Our model can transform random normal noise to raw waveforms
                given mel-spectrograms in parallel.
              </td>
            </tr>
          </tbody>
        </table>
      </div>
    </article>
  </div>
</div>
